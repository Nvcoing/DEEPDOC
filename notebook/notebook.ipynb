{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6a6c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from pypdf import PdfReader\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from docx import Document\n",
    "from pptx import Presentation\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 1. Load file (PDF, DOCX, PPTX)\n",
    "# ================================\n",
    "def load_file_pages(path):\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "\n",
    "    # ========== PDF ==========\n",
    "    if ext == \".pdf\":\n",
    "        reader = PdfReader(path)\n",
    "        pages = []\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text() or \"\"\n",
    "            pages.append(text)\n",
    "        return pages\n",
    "\n",
    "    # ========== DOCX ==========\n",
    "    elif ext == \".docx\":\n",
    "        doc = Document(path)\n",
    "        pages = []\n",
    "\n",
    "        buffer = []\n",
    "        paragraph_count = 0\n",
    "\n",
    "        for para in doc.paragraphs:\n",
    "            text = para.text.strip()\n",
    "            if text:\n",
    "                buffer.append(text)\n",
    "                paragraph_count += 1\n",
    "\n",
    "            # Cho thành \"page\" sau mỗi 20 đoạn (tùy chỉnh)\n",
    "            if paragraph_count >= 20:\n",
    "                pages.append(\"\\n\".join(buffer))\n",
    "                buffer = []\n",
    "                paragraph_count = 0\n",
    "\n",
    "        if buffer:\n",
    "            pages.append(\"\\n\".join(buffer))\n",
    "\n",
    "        return pages\n",
    "\n",
    "    # ========== PPTX ==========\n",
    "    elif ext == \".pptx\":\n",
    "        pres = Presentation(path)\n",
    "        pages = []\n",
    "\n",
    "        for slide in pres.slides:\n",
    "            slide_text = []\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\"):\n",
    "                    slide_text.append(shape.text)\n",
    "            pages.append(\"\\n\".join(slide_text))\n",
    "\n",
    "        return pages\n",
    "\n",
    "    # ========== Không hỗ trợ ==========\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format: only PDF, DOCX, PPTX.\")\n",
    "\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 2. Chunk 1 trang\n",
    "# ================================\n",
    "def chunk_page(text, chunk_size=800, overlap=200):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 3. Build chunks + metadata\n",
    "# ================================\n",
    "def build_chunks(path):\n",
    "    pages = load_file_pages(path)\n",
    "\n",
    "    all_chunks = []\n",
    "    all_ids = []\n",
    "    all_meta = []\n",
    "\n",
    "    for page_idx, text in enumerate(pages):\n",
    "        page_number = page_idx + 1\n",
    "        chunks = chunk_page(text)\n",
    "\n",
    "        for ci, c in enumerate(chunks):\n",
    "            all_chunks.append(c)\n",
    "            all_ids.append(f\"{os.path.basename(path)}_p{page_number}_c{ci}\")\n",
    "            all_meta.append({\n",
    "                \"page\": page_number,\n",
    "                \"chunk\": ci\n",
    "            })\n",
    "\n",
    "    return all_chunks, all_ids, all_meta, pages\n",
    "\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 4. Model + Vector DB\n",
    "# ================================\n",
    "model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n",
    "\n",
    "chroma = chromadb.Client()\n",
    "collection = chroma.get_or_create_collection(\n",
    "    name=\"pdf_docs\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 5. Index PDF/DOCX/PPTX\n",
    "# ================================\n",
    "def index_pdf(path, batch_size=2):\n",
    "    chunks, ids, metadata, pages = build_chunks(path)\n",
    "\n",
    "    all_embeds = []\n",
    "\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i : i + batch_size]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            vec = model.encode(batch).tolist()\n",
    "\n",
    "        all_embeds.extend(vec)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Đã embed {i + len(batch)}/{len(chunks)} chunks\", end=\"\\r\")\n",
    "\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        documents=chunks,\n",
    "        metadatas=metadata,\n",
    "        embeddings=all_embeds\n",
    "    )\n",
    "\n",
    "    print(f\"\\nIndexed {len(chunks)} chunks từ file {path}\")\n",
    "    return pages\n",
    "\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 6. Lấy trang lân cận\n",
    "# ================================\n",
    "def get_surrounding_pages(page, pages):\n",
    "    prev_page = pages[page - 2] if page > 1 else None\n",
    "    this_page = pages[page - 1]\n",
    "    next_page = pages[page] if page < len(pages) else None\n",
    "\n",
    "    return prev_page, this_page, next_page\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. SEARCH 2 BƯỚC: TopK → Expand → Re-chunk → Rerank\n",
    "# ============================================================\n",
    "def search(query, pages, top_k_first=3, top_k_second=5, chunk_size=800, overlap=200):\n",
    "\n",
    "    # STEP 1 ───────────────────────────────────────────────\n",
    "    q_emb = model.encode([query]).tolist()\n",
    "\n",
    "    result = collection.query(\n",
    "        query_embeddings=q_emb,\n",
    "        n_results=top_k_first,\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "\n",
    "    metas = result[\"metadatas\"][0]\n",
    "\n",
    "    # STEP 2 ───────────────────────────────────────────────\n",
    "    expanded_pages = []\n",
    "    seen = set()\n",
    "\n",
    "    for meta in metas:\n",
    "        page = meta[\"page\"]\n",
    "        prev_page, this_page, next_page = get_surrounding_pages(page, pages)\n",
    "        candidates = [prev_page, this_page, next_page]\n",
    "\n",
    "        for p in candidates:\n",
    "            if p and p not in seen:\n",
    "                expanded_pages.append(p)\n",
    "                seen.add(p)\n",
    "\n",
    "    # STEP 3 ───────────────────────────────────────────────\n",
    "    big_text = \"\\n\\n\".join(expanded_pages)\n",
    "    words = big_text.split()\n",
    "\n",
    "    re_chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        re_chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "\n",
    "    # STEP 4 ───────────────────────────────────────────────\n",
    "    embeds = model.encode(re_chunks).tolist()\n",
    "\n",
    "    temp = chromadb.Client().create_collection(\n",
    "        name=\"temp_rerank\",\n",
    "        metadata={\"hnsw:space\": \"cosine\"},\n",
    "        get_or_create=True\n",
    "    )\n",
    "\n",
    "    temp_ids = [f\"rechunk_{i}\" for i in range(len(re_chunks))]\n",
    "    temp.add(ids=temp_ids, embeddings=embeds, documents=re_chunks)\n",
    "\n",
    "    result2 = temp.query(\n",
    "        query_embeddings=q_emb,\n",
    "        n_results=top_k_second,\n",
    "        include=[\"documents\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    docs2 = result2[\"documents\"][0]\n",
    "    dists2 = result2[\"distances\"][0]\n",
    "\n",
    "    # STEP 5 ───────────────────────────────────────────────\n",
    "    output = []\n",
    "    for i, (doc, dist) in enumerate(zip(docs2, dists2), start=1):\n",
    "        output.append({\n",
    "            \"rank\": i,\n",
    "            \"matched_chunk\": doc,\n",
    "            \"score\": 1 - dist,\n",
    "            \"raw_distance\": dist\n",
    "        })\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # 8. RUN\n",
    "# # ============================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     pages = index_pdf(\"test.docx\")   # hoặc test.pdf, test.pptx\n",
    "\n",
    "#     results = search(\"Mô hình nào được dùng?\", pages)\n",
    "\n",
    "#     for r in results:\n",
    "#         print(\"=\"*80)\n",
    "#         print(\"Rank:\", r[\"rank\"])\n",
    "#         print(\"Score:\", r[\"score\"])\n",
    "#         print(r[\"matched_chunk\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "913e8dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã embed 20/20 chunks\n",
      "Indexed 20 chunks từ file test2.docx\n"
     ]
    }
   ],
   "source": [
    "pages = index_pdf(\"test2.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eafbb82d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mĐiều 20. Trách nhiệm triển khai Quy chế\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k_first\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k_second\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlap\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 193\u001b[39m, in \u001b[36msearch\u001b[39m\u001b[34m(query, pages, top_k_first, top_k_second, chunk_size, overlap)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m meta \u001b[38;5;129;01min\u001b[39;00m metas:\n\u001b[32m    192\u001b[39m     page = meta[\u001b[33m\"\u001b[39m\u001b[33mpage\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     prev_page, this_page, next_page = \u001b[43mget_surrounding_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m     candidates = [prev_page, this_page, next_page]\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m candidates:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 163\u001b[39m, in \u001b[36mget_surrounding_pages\u001b[39m\u001b[34m(page, pages)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_surrounding_pages\u001b[39m(page, pages):\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     prev_page = \u001b[43mpages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m page > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    164\u001b[39m     this_page = pages[page - \u001b[32m1\u001b[39m]\n\u001b[32m    165\u001b[39m     next_page = pages[page] \u001b[38;5;28;01mif\u001b[39;00m page < \u001b[38;5;28mlen\u001b[39m(pages) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "results = search(\"Điều 20. Trách nhiệm triển khai Quy chế\", pages, top_k_first=3, top_k_second=5, chunk_size=300, overlap=100)\n",
    "\n",
    "for r in results:\n",
    "    print(\"=\"*80)\n",
    "    print(\"Rank:\", r[\"rank\"])\n",
    "    print(\"Score:\", r[\"score\"])\n",
    "    print(r[\"matched_chunk\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
