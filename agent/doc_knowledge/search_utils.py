import torch
import gc
from typing import List, Dict, Set
from qdrant_client.models import Filter, FieldCondition, MatchValue
from doc_knowledge.entities import extract_entities, highlight_markdown
from doc_knowledge.config import embed_model, rank_model, device, CLIENT

class DOCSearcher:
    def __init__(
        self,
        collections: List[str],
        page_topk=10,          # S·ªë page ƒë·ªÉ search
        chunk_topk=5           # S·ªë chunk t·ªët nh·∫•t sau rerank
    ):
        self.collections = collections
        self.page_topk = page_topk
        self.chunk_topk = chunk_topk

    def _scroll_page(self, collection: str, pid: int) -> str:
        """L·∫•y full text c·ªßa page t·ª´ vector DB"""
        res = CLIENT.scroll(
            collection_name=collection,
            scroll_filter=Filter(must=[
                FieldCondition(key="type", match=MatchValue(value="page")),
                FieldCondition(key="page", match=MatchValue(value=pid))
            ]),
            with_payload=True
        )
        return res[0][0].payload.get("text", "") if res[0] else ""

    def _get_chunks_for_pages(
        self, 
        collection: str, 
        page_ids: Set[int]
    ) -> Dict[tuple, str]:
        """
        L·∫•y t·∫•t c·∫£ chunks c·ªßa c√°c pages ƒë√£ ch·ªçn.
        Return: {(collection, page_id, chunk_id): chunk_text}
        """
        chunks = {}
        
        for pid in page_ids:
            res = CLIENT.scroll(
                collection_name=collection,
                scroll_filter=Filter(must=[
                    FieldCondition(key="type", match=MatchValue(value="chunk")),
                    FieldCondition(key="page", match=MatchValue(value=pid))
                ]),
                with_payload=True,
                limit=100  # M·ªói page t·ªëi ƒëa 3 chunks
            )
            
            for point in res[0]:
                payload = point.payload
                chunk_id = payload.get("chunk_id", 0)
                chunk_text = payload.get("text", "")
                if chunk_text:
                    chunks[(collection, pid, chunk_id)] = chunk_text
        
        return chunks

    def _rerank(self, query: str, items: dict, topk: int) -> list:
        """Rerank c√°c items b·∫±ng cross-encoder"""
        if not items:
            return []
        
        pairs = [(query, text) for text in items.values()]
        scores = rank_model.predict(pairs, batch_size=8)
        
        ranked = sorted(
            zip(items.keys(), scores),
            key=lambda x: x[1],
            reverse=True
        )
        return ranked[:topk]

    def search(self, query: str):
        # ===== 1. EMBED QUERY 1 L·∫¶N =====
        with torch.no_grad():
            q_emb = embed_model.encode(
                [query],
                normalize_embeddings=True
            ).tolist()[0]

        global_pages = {}  # {(collection, page_id): page_text}
        
        # ===== 2. SEARCH TOP PAGES (10 PAGES) =====
        print(f"\nüîç ƒêang search top {self.page_topk} pages...")
        
        for col in self.collections:
            try:
                page_results = CLIENT.search(
                    collection_name=col,
                    query_vector=q_emb,
                    query_filter=Filter(
                        must=[FieldCondition(key="type", match=MatchValue(value="page"))]
                    ),
                    limit=self.page_topk,
                    with_payload=True
                )

                for p in page_results:
                    pid = p.payload["page"]
                    page_text = p.payload.get("text", "")
                    if page_text:
                        global_pages[(col, pid)] = page_text
                        
            except Exception as e:
                print(f"‚ö†Ô∏è  Search error in {col}: {e}")

        print(f"‚úì T√¨m th·∫•y {len(global_pages)} pages t·ª´ {len(self.collections)} collections")

        if not global_pages:
            print("‚ùå Kh√¥ng t√¨m th·∫•y pages n√†o!")
            return []

        # ===== 3. L·∫§Y T·∫§T C·∫¢ CHUNKS C·ª¶A C√ÅC PAGES ƒê√É CH·ªåN =====
        print(f"\nüì¶ ƒêang l·∫•y chunks c·ªßa {len(global_pages)} pages...")
        
        all_chunks = {}  # {(collection, page_id, chunk_id): chunk_text}
        
        for col in self.collections:
            # L·∫•y page_ids thu·ªôc collection n√†y
            page_ids = {pid for (c, pid) in global_pages.keys() if c == col}
            
            if page_ids:
                chunks = self._get_chunks_for_pages(col, page_ids)
                all_chunks.update(chunks)
        
        print(f"‚úì Thu th·∫≠p ƒë∆∞·ª£c {len(all_chunks)} chunks")

        # ===== 4. RERANK CHUNKS =====
        print(f"\nüéØ ƒêang rerank {len(all_chunks)} chunks...")
        
        ranked_chunks = self._rerank(query, all_chunks, self.chunk_topk)
        
        print(f"‚úì Ch·ªçn top {len(ranked_chunks)} chunks c√≥ ƒëi·ªÉm cao nh·∫•t")

        # ===== 5. T·∫†O OUTPUT THEO CHUNKS (KH√îNG THEO PAGES) =====
        outputs = []
        
        for rank, ((col, pid, chunk_id), score) in enumerate(ranked_chunks, start=1):
            chunk_text = all_chunks[(col, pid, chunk_id)]
            page_text = global_pages[(col, pid)]
            
            # Highlight entities trong chunk
            highlighted_chunk = highlight_markdown(
                chunk_text, 
                extract_entities(chunk_text)
            )
            
            outputs.append({
                "rank": rank,
                "collection": col,
                "page": pid + 1,  # Hi·ªÉn th·ªã page number (1-indexed)
                "chunk_id": chunk_id + 1,  # 1, 2, ho·∫∑c 3 (ph·∫ßn ƒë·∫ßu/gi·ªØa/cu·ªëi)
                "score": round(float(score), 4),
                "chunk_text": chunk_text,
                "highlighted_chunk": highlighted_chunk,
                "full_page_text": page_text  # C√≥ th·ªÉ d√πng ƒë·ªÉ context
            })

        # ===== 6. CLEANUP MEMORY =====
        gc.collect()
        if device == "cuda":
            torch.cuda.empty_cache()

        print(f"\n‚úÖ Ho√†n th√†nh! Tr·∫£ v·ªÅ {len(outputs)} k·∫øt qu·∫£ t·ªët nh·∫•t")
        return outputs